Natural Language Processing with Python
=======================================

Chapter 1
=========
Language Processing and Python

nltk.download() to download all corpus for the book.

corpus: nltk.book

Searching text:
concordance() - works like grep
Usage: text1.concordance(word)
Make sure to create a nltk.text object of a corpus to use concordance function on it.

similar() - Showing words having similar context in the corpus
Usage: text1.similar(word)

common_context() - Showing context shared by two or more words
Usage: text1.common_context([word1, word2, ...])

dispersion_plot() - Showing positional information of word in a corpus
Usage: text1.dispersion_plot([word1, word2, ...])

generate() - Generate random text from the corpus
Usage: text1.generate() - Not found in nltk 3.0

len() - find the lenght of a string
Usage: len(string)

set() - find distinct words
Usage: set(string)

Lexical richness of the text:
len(string) / len(set(string))

count() - count the occurance of a word
Usage: string.count(word)

FreqDist() - Output the frequency distribution in dict format
Usage: FreqDist(string)

Cumulative frequency plot:
Usage: dict.plot(count, cumulative=True)

collocations() - Bigrams that occur more often
Usage: text4.collocations()

Challenges in NLP:
a. Word Sense Disambiguation:
For e.g He serve the dish.

b. Pronoun Resolution:
For e.g The thieves stole the paintings. They were sold.

c. Generating Language Output:
Usage: Question answering and Machine Translation

d. Machine Translation:
babelize_shell() - To make the translation back and forth between two languages
Note: No longer works

e. Spoken Dialogue Systems:
Siri or Google Now

f. Textual Entailment:
Directional relation between text fragments
For e.g
Text: David Golinkin is the editor or author of 18 books, and over 150 responsa, articles, sermons and books
Hypothesis: Golinkin has written 18 books

===============================================================================================================

Chapter 2
=========
Accessing Text Corpora and Lexical Resources

a. Gutenberg Corpus: Contains thousands of books
nltk.corpus.gutenberg

Functions:
fileids()
Usage: gutenberg.fileids()

words()
Usage: gutenberg.words(fileid)

raw()
Usage: gutenberg.raw(fileid)

sents()
Usage: gutenberg.sents(fileid)

b. Web and Chat Text:
nltk.corpus.webtext

c. NPS Chat:
nltk.corpus.nps_chat

d. Brown Corpus:
nlkt.corpus.brown

Functions:
categories()
Usage: brown.categories()

brown.words(categories=category)

nlkt.ConditionalFreqDist()

e. Reuters Corpus:
Training and test dataset
nltk.corpus.reuters

Functions:
reuters.categories(fileid)
reuters.fileids(category)

f. Inaugural Address Corpus
nltk.corpus.inaugural

g. Annotated Text Corpora
100 of corpora

h. Corpora in other languages

More functions:
a. abspath(fileid)
b. encoding(fileid)
c. open(fileid)
d. root()
e. readme()

Text Corpus Structure:
Four types of structures
a. Isolated
b. Categorized
c. Overlapping
d. Temporal

Loading your own corpus:
Using two libraries
textCorpusReader
BracketParseCorpusReader

Passing corpus root, and file pattern as params.

Conditional Frequency Distributions:
Frequency distributions amoung several categories

Usage:
cfd = nltk.ConditionalFreqDist(
	(target, fileid[:4])
	for fileid in inaugural.fileids() 
	for w in inaugural.words(fileid)
	for target in ['america', 'citizen'] 
	if w.lower().startswith(target))

Generating random text with Bigrams:
Using nltk.bigrams(corpus), and ConditionalFreqDist(bigrams) to get the highest probability bigram, generate random text.

Lexical Resources:
Lexicon, collection of words/phrases along with associated information, such as POS, head word.

Various Lexicons:
a. Wordlist Corpora
nltk.corpus.words
nltk.corpus.stopwords
nltk.corpus.names

b. A Pronouncing Dictionary
nlkt.corpus.cmudist

c. Comparative Wordlists - 200 common words in several languages
nltk.corpus import swadesh

d. Shoebox and Toolbox lexicons
nltk.corpus.toolbox
